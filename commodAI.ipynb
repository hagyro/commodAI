{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# commodAI: Commodity Market Analysis\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1.  **Primary Goal:** Develop a Python-based (Jupyter Notebook) system for analyzing a commodity market dataset (2000-2023 closing ticks for 18 commodities) to identify and cluster periods of market instability.\n",
    "2.  **Anomaly Detection:** Utilize machine learning (and potentially deep learning) techniques to detect anomalies within the time series data of each commodity.  Anomalies should be tagged with confidence levels.  Focus on identifying regions, not just individual points.\n",
    "3.  **Clustering:** Perform ensemble clustering on the identified anomaly regions.  Provide statistical justification for the chosen clustering methods and the resulting clusters.  Analyze and explain the characteristics of each cluster.\n",
    "4.  **Visualization:** Create a modern, interactive dashboard (within the Jupyter Notebook environment) to visualize the time series data, detected anomalies, clustering results, and statistical analyses.\n",
    "5.  **Self-Contained Execution:** The entire project MUST be executable on a single machine (MacBook M1 Pro, Sonoma 14.7.3, Anaconda environment, Python 3.9.7) without relying on external web services or APIs (beyond standard package installations).\n",
    "6.  **Reproducibility:** The jupyter notbook must provide ALL the necessary setup for the environment to be replicated.\n",
    "7. **Document:** The jupyter notebook must contain a balanced mixture of code and markdown cells to document the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae7197",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section describes the steps to set up the Anaconda environment and install the necessary packages.  It is crucial to follow these steps to ensure the notebook can be executed successfully and reproducibly.\n",
    "\n",
    "We will create a dedicated Anaconda environment named `commodAI_env` with Python 3.9.7.  This isolates the project's dependencies and avoids conflicts with other Python projects.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Create the Anaconda environment:**\n",
    "    *   Open your terminal.\n",
    "    *   Run the following command:\n",
    "    ```bash\n",
    "    conda create -n commodAI_env python=3.9.7\n",
    "    ```\n",
    "    This command creates an environment named `commodAI_env` with Python version 3.9.7.\n",
    "\n",
    "2.  **Activate the environment:**\n",
    "    *   In the terminal, run:\n",
    "    ```bash\n",
    "    conda activate commodAI_env\n",
    "    ```\n",
    "    This activates the `commodAI_env` environment, making it the active Python environment for your terminal.\n",
    "\n",
    "3.  **Install the required packages:**\n",
    "    *   You can install the packages individually using `pip install`, or you can use the provided `requirements.txt` file.\n",
    "    *   To install using `requirements.txt`, first ensure the file is in the same directory as your Jupyter Notebook.\n",
    "    *   Then, in the terminal (with the environment activated), run:\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "    This command installs all the packages listed in `requirements.txt` along with their dependencies.\n",
    "\n",
    "**requirements.txt:**\n",
    "```\n",
    "pandas==1.5.3\n",
    "scikit-learn==1.2.2\n",
    "plotly==5.14.1\n",
    "tensorflow==2.12.0\n",
    "ipywidgets==8.0.4\n",
    "scipy==1.10.1\n",
    "```\n",
    "\n",
    "These are the core packages required for this project.  Specific versions are listed to ensure reproducibility.  You can create this file manually or use `conda list -e > requirements.txt` to generate it from your environment after installing the packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcb9f6",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section loads the commodity market data from the CSV file (`Gran Canaria_database_v3.csv`) and performs initial data preprocessing.\n",
    "\n",
    "The dataset contains daily closing prices for 18 different commodities from 2000 to 2023.  The goal of preprocessing is to clean the data, handle missing values, and prepare it for anomaly detection and clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "df = pd.read_csv(\"database_v3.csv\", sep=';')\n",
    "\n",
    "# Ensure the 'date' column is in the correct datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values using interpolation\n",
    "df = df.interpolate(method='ffill', limit_direction='forward')\n",
    "print(\"\\nMissing values after interpolation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic check for outliers using z-score (example for the 'Crude oil, Brent-Europe' column)\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "commodity_column = [\"crude\", \"brent\"]\n",
    "z_scores = np.abs(stats.zscore(df[commodity_column]))\n",
    "outlier_threshold = 3 # Z-score threshold for outlier detection\n",
    "outliers = df[z_scores > outlier_threshold]\n",
    "\n",
    "print(f\"\\nNumber of outliers in '{commodity_column}' column (Z-score > {outlier_threshold}): {len(outliers)}\")\n",
    "if not outliers.empty:\n",
    "    print(\"Outlier indices:\", outliers.index.tolist())\n",
    "    print(\"First few outlier values:\")\n",
    "    print(outliers.head())\n",
    "else:\n",
    "    print(\"No outliers detected in '{commodity_column}' column based on Z-score threshold.\")\n",
    "\n",
    "# Further data consistency checks can be added here if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import pandas\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763bec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import pandas\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d051de",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "This section focuses on detecting anomalies in the commodity time series data. We use the Isolation Forest algorithm, which is an unsupervised learning method particularly effective for anomaly detection in high-dimensional datasets.\n",
    "\n",
    "**Isolation Forest:**\n",
    "Isolation Forest isolates anomalies by randomly partitioning the data. Anomalies are easier to isolate and therefore require fewer partitions. The `contamination` parameter specifies the expected proportion of outliers in the dataset.  It's important to choose this parameter carefully based on your understanding of the data.  A higher value will result in more data points being flagged as anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3efb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import pandas\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f3885",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import pandas\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IsolationForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Define a function to detect anomalies for each commodity\n",
    "def detect_anomalies(df, commodity, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detects anomalies in a given commodity's time series data using Isolation Forest.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the commodity data.\n",
    "        commodity (str): The name of the commodity column.\n",
    "        contamination (float): The proportion of outliers in the data set.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the anomaly scores and anomaly regions.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    # Train Isolation Forest model\n",
    "    model = IsolationForest(contamination=contamination, random_state=42)\n",
    "    model.fit(df_copy[[commodity]])\n",
    "    # Get anomaly scores\n",
    "    scores = model.decision_function(df_copy[[commodity]])\n",
    "    # Get anomaly predictions\n",
    "    predictions = model.predict(df_copy[[commodity]])\n",
    "    # Identify anomaly regions\n",
    "    anomaly_regions = []\n",
    "    start_index = None\n",
    "    # Iterate through the predictions to identify anomaly regions\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        # If the current prediction is an anomaly and we haven't started an anomaly region yet\n",
    "        if prediction == -1 and start_index is None:\n",
    "            # Start a new anomaly region\n",
    "            start_index = i\n",
    "        # If the current prediction is not an anomaly and we have started an anomaly region\n",
    "        elif prediction == 1 and start_index is not None:\n",
    "            # End the anomaly region\n",
    "            anomaly_regions.append((start_index, i - 1))\n",
    "            # Reset the start index\n",
    "            start_index = None\n",
    "    # If we started an anomaly region but didn't end it\n",
    "    if start_index is not None:\n",
    "        # End the anomaly region at the end of the data\n",
    "        anomaly_regions.append((start_index, len(predictions) - 1))\n",
    "    return scores, anomaly_regions\n",
    "\n",
    "# Apply anomaly detection to each commodity\n",
    "commodity_columns = df.columns[1:]  # Exclude the 'date' column\n",
    "anomaly_scores = {}\n",
    "anomaly_regions = {}\n",
    "for commodity in commodity_columns:\n",
    "    scores, regions = detect_anomalies(df, commodity)\n",
    "    anomaly_scores[commodity] = scores\n",
    "    anomaly_regions[commodity] = regions\n",
    "    print(f\"\\nAnomaly regions for '{commodity}': {regions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981268d",
   "metadata": {},
   "source": [
    "## Ensemble Clustering\n",
    "\n",
    "This section implements ensemble clustering on the identified anomaly regions using k-means, DBSCAN, and hierarchical clustering.\n",
    "\n",
    "**Why Ensemble Clustering?**\n",
    "Ensemble clustering combines the results of multiple clustering algorithms to obtain a more robust and stable clustering solution.  Different algorithms have different strengths and weaknesses, and combining them can help to overcome the limitations of individual algorithms. In this case, we are using K-means, DBSCAN and Hierarchical clustering. However, the results are not combined, and each algorithm is evaluated independently.\n",
    "\n",
    "It evaluates the quality of the clusters using Silhouette score and Davies-Bouldin index, and analyzes the characteristics of each cluster.\n",
    "\n",
    "**Clustering Methods:**\n",
    "*   **K-means:** A centroid-based clustering algorithm that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.\n",
    "*   **DBSCAN:** A density-based clustering algorithm that groups together points that are closely packed together, marking as outliers points that lie alone in low-density regions.\n",
    "*   **Hierarchical Clustering:** A clustering algorithm that builds a hierarchy of clusters. It starts with each data point in its own cluster, and then merges the closest pairs of clusters until only a single cluster remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clustering algorithms\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "# Import metrics for evaluation\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "# Prepare data for clustering\n",
    "anomaly_data = []\n",
    "for commodity in commodity_columns:\n",
    "    regions = anomaly_regions[commodity]\n",
    "    for start, end in regions:\n",
    "        # Extract the data for the anomaly region\n",
    "        region_data = df[commodity][start:end+1].values\n",
    "        anomaly_data.append(region_data)\n",
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_anomaly_data = pad_sequences(anomaly_data, padding='post', dtype='float64')\n",
    "# Define the clustering algorithms\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init = 'auto')\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "hierarchical = AgglomerativeClustering(n_clusters=3)\n",
    "# Fit the clustering algorithms\n",
    "kmeans_labels = kmeans.fit_predict(padded_anomaly_data)\n",
    "dbscan_labels = dbscan.fit_predict(padded_anomaly_data)\n",
    "hierarchical_labels = hierarchical.fit_predict(padded_anomaly_data)\n",
    "# Evaluate the clustering algorithms\n",
    "def evaluate_clustering(labels, data):\n",
    "    \"\"\"\n",
    "    Evaluates the clustering algorithms using Silhouette score and Davies-Bouldin index.\n",
    "    Args:\n",
    "        labels (np.ndarray): The cluster labels.\n",
    "        data (np.ndarray): The data used for clustering.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the Silhouette score and Davies-Bouldin index.\n",
    "    \"\"\"\n",
    "    # Filter out noise labels (-1 for DBSCAN)\n",
    "    if -1 in labels:\n",
    "        data = data[labels != -1]\n",
    "        labels = labels[labels != -1]\n",
    "    if len(set(labels)) < 2:\n",
    "        return -1, -1\n",
    "    silhouette = silhouette_score(data, labels)\n",
    "    davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    return silhouette, davies_bouldin\n",
    "\n",
    "kmeans_silhouette, kmeans_davies_bouldin = evaluate_clustering(kmeans_labels, padded_anomaly_data)\n",
    "dbscan_silhouette, dbscan_davies_bouldin = evaluate_clustering(dbscan_labels, padded_anomaly_data)\n",
    "hierarchical_silhouette, hierarchical_davies_bouldin = evaluate_clustering(hierarchical_labels, padded_anomaly_data)\n",
    "\n",
    "print(f\"K-means Silhouette score: {kmeans_silhouette}, Davies-Bouldin index: {kmeans_davies_bouldin}\")\n",
    "print(f\"DBSCAN Silhouette score: {dbscan_silhouette}, Davies-Bouldin index: {dbscan_davies_bouldin}\")\n",
    "print(f\"Hierarchical Silhouette score: {hierarchical_silhouette}, Davies-Bouldin index: {hierarchical_davies_bouldin}\")\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "from collections import defaultdict\n",
    "cluster_characteristics = defaultdict(list)\n",
    "for i, label in enumerate(kmeans_labels):\n",
    "    commodity = commodity_columns[i % len(commodity_columns)]\n",
    "    cluster_characteristics[label].append(commodity)\n",
    "\n",
    "print(\"\\nCluster characteristics:\")\n",
    "for label, commodities in cluster_characteristics.items():\n",
    "    print(f\"Cluster {label}: {commodities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281169b",
   "metadata": {},
   "source": [
    "## Interactive Dashboard\n",
    "\n",
    "This section creates an interactive dashboard to visualize the time series data, detected anomalies, and clustering results.\n",
    "\n",
    "The dashboard allows the user to select a commodity from a dropdown menu and view the corresponding time series data, anomaly regions, and clustering results.  The anomaly regions are highlighted in red, and the clusters are color-coded to distinguish them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a dropdown widget for commodity selection\n",
    "commodity_dropdown = widgets.Dropdown(\n",
    "    options=commodity_columns,\n",
    "    description='Select Commodity:'\n",
    ")\n",
    "# Create an output widget for the plot\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Define a function to update the plot based on the selected commodity\n",
    "def update_plot(commodity):\n",
    "    with plot_output:\n",
    "        plot_output.clear_output()\n",
    "        # Create the plot\n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        # Add time series data\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df['date'], y=df[commodity], mode='lines', name=commodity),\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        # Add anomaly regions\n",
    "        for start, end in anomaly_regions[commodity]:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df['date'][start:end+1], y=df[commodity][start:end+1], mode='lines', name='Anomaly Region', line=dict(color='red')),\n",
    "                secondary_y=False,\n",
    "            )\n",
    "        # Add clustering results (example: using KMeans labels)\n",
    "        # Note: This is a simplified example. You'll need to map the anomaly regions to the cluster labels.\n",
    "        # Assuming kmeans_labels is a list of cluster labels for each anomaly region\n",
    "        # and anomaly_regions[commodity] is a list of (start, end) tuples for each anomaly region.\n",
    "        # You'll need to iterate through the anomaly regions and assign a color based on the cluster label.\n",
    "        colors = ['blue', 'green', 'purple']  # Example colors for clusters\n",
    "        for i, (start, end) in enumerate(anomaly_regions[commodity]):\n",
    "            cluster_label = kmeans_labels[i % len(kmeans_labels)]  # Get the cluster label for the anomaly region\n",
    "            color = colors[cluster_label % len(colors)]  # Assign a color based on the cluster label\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df['date'][start:end+1], y=df[commodity][start:end+1], mode='lines', name=f'Cluster {cluster_label}', line=dict(color=color)),\n",
    "                secondary_y=False,\n",
    "            )\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Commodity: {commodity} with Anomaly Regions and Clustering',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Price',\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "# Link the dropdown to the update function\n",
    "commodity_dropdown.observe(lambda change: update_plot(change.new), names=['value'])\n",
    "\n",
    "# Display the widgets\n",
    "display(commodity_dropdown)\n",
    "display(plot_output)\n",
    "\n",
    "# Initialize the plot with the first commodity\n",
    "update_plot(commodity_columns[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

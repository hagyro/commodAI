{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# commodAI: Commodity Market Analysis\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1.  **Primary Goal:** Develop a Python-based (Jupyter Notebook) system for analyzing a commodity market dataset (2000-2023 closing ticks for 18 commodities) to identify and cluster periods of market instability.\n",
    "2.  **Anomaly Detection:** Utilize machine learning (and potentially deep learning) techniques to detect anomalies within the time series data of each commodity.  Anomalies should be tagged with confidence levels.  Focus on identifying regions, not just individual points.\n",
    "3.  **Clustering:** Perform ensemble clustering on the identified anomaly regions.  Provide statistical justification for the chosen clustering methods and the resulting clusters.  Analyze and explain the characteristics of each cluster.\n",
    "4.  **Visualization:** Create a modern, interactive dashboard (within the Jupyter Notebook environment) to visualize the time series data, detected anomalies, clustering results, and statistical analyses.\n",
    "5.  **Self-Contained Execution:** The entire project MUST be executable on a single machine (MacBook M1 Pro, Sonoma 14.7.3, Anaconda environment, Python 3.9.7) without relying on external web services or APIs (beyond standard package installations).\n",
    "6.  **Reproducibility:** The jupyter notbook must provide ALL the necessary setup for the environment to be replicated.\n",
    "7. **Document:** The jupyter notebook must contain a balanced mixture of code and markdown cells to document the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcb9f6",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section loads the commodity market data from the CSV file (`Gran Canaria_database_v3.csv`) and performs initial data preprocessing.\n",
    "\n",
    "The dataset contains daily closing prices for 18 different commodities from 2000 to 2023.  The goal of preprocessing is to clean the data, handle missing values, and prepare it for anomaly detection and clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf2d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5803, 19)\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "   crude  brent  gasoline  ngas  heating  diesel    corn  soybeans  sugar  \\\n",
      "0  25.55  23.95     0.658  2.16    0.687   0.820  203.00    464.25   5.89   \n",
      "1  24.93  23.72     0.649  2.17    0.671   0.818  203.00    469.25   6.18   \n",
      "2  24.78  23.55     0.659  2.18    0.675   0.795  203.75    468.00   5.93   \n",
      "3  24.23  23.35     0.640  2.19    0.660   0.795  207.00    471.50   5.87   \n",
      "4  24.68  22.77     0.646  2.20    0.660   0.795  208.50    466.25   5.87   \n",
      "\n",
      "     gold     snp  silver  platinum  palladium   copper     tin  nickel  \\\n",
      "0  281.25  381.78    5.30     433.0      443.0  1829.75  5770.0  8295.0   \n",
      "1  280.50  378.07    5.18     420.0      433.0  1840.00  5950.0  8289.0   \n",
      "2  279.40  377.61    5.17     414.0      433.0  1833.75  5950.0  8238.5   \n",
      "3  282.30  378.56    5.18     418.0      438.0  1841.40  6070.0  8162.0   \n",
      "4  281.70  378.65    5.20     418.0      449.0  1822.50  6060.0  8120.0   \n",
      "\n",
      "   aluminum  \n",
      "0   1611.25  \n",
      "1   1630.50  \n",
      "2   1635.25  \n",
      "3   1648.00  \n",
      "4   1633.45  \n",
      "\n",
      "Descriptive statistics:\n",
      "             crude        brent     gasoline         ngas      heating  \\\n",
      "count  5798.000000  5798.000000  5798.000000  5786.000000  5798.000000   \n",
      "mean     62.684536    65.593962     1.827486     4.485812     1.875223   \n",
      "std      26.117750    29.648386     0.756167     2.181771     0.849561   \n",
      "min     -37.630000     9.120000     0.434000     1.470000     0.467000   \n",
      "25%      41.617500    42.322500     1.238000     2.860000     1.205000   \n",
      "50%      59.950000    62.595000     1.768000     3.900000     1.800500   \n",
      "75%      83.105000    85.845000     2.376000     5.630000     2.483750   \n",
      "max     145.660000   143.950000     4.509000    18.480000     5.152000   \n",
      "\n",
      "            diesel         corn     soybeans        sugar         gold  \\\n",
      "count  5797.000000  5798.000000  5798.000000  5798.000000  5798.000000   \n",
      "mean      1.985719   399.595429   971.985029    14.334062  1051.553667   \n",
      "std       0.835797   164.156203   339.884299     5.451364   523.379359   \n",
      "min       0.490000   174.750000   418.000000     4.700000   256.000000   \n",
      "25%       1.330000   248.062500   651.250000    10.050000   467.000000   \n",
      "50%       1.935000   367.500000   952.625000    13.650000  1195.970000   \n",
      "75%       2.557000   495.937500  1262.875000    17.980000  1414.912500   \n",
      "max       4.719000   831.250000  1771.000000    32.570000  2056.100000   \n",
      "\n",
      "               snp       silver     platinum    palladium        copper  \\\n",
      "count  5798.000000  5798.000000  5798.000000  5798.000000   5798.000000   \n",
      "mean   1328.058343    15.992009  1074.278663   804.638263   5726.431256   \n",
      "std     617.137771     8.462581   370.098086   647.145503   2480.202736   \n",
      "min     358.390000     4.050000   414.000000   146.000000   1318.250000   \n",
      "25%     642.917500     7.787500   843.000000   336.000000   3602.250000   \n",
      "50%    1481.395000    16.160000   985.500000   650.000000   6228.625000   \n",
      "75%    1792.922500    20.040000  1355.000000   884.750000   7521.875000   \n",
      "max    2594.130000    48.550000  2273.000000  3015.000000  11299.500000   \n",
      "\n",
      "                tin        nickel     aluminum  \n",
      "count   5798.000000   5798.000000  5798.000000  \n",
      "mean   16397.620423  16081.683888  1964.569953  \n",
      "std     8541.014480   7856.964554   461.592456  \n",
      "min     3601.000000   4346.000000     0.000000  \n",
      "25%     8614.250000  10554.812500  1622.625000  \n",
      "50%    17217.750000  14595.000000  1870.250000  \n",
      "75%    21186.500000  19063.125000  2256.000000  \n",
      "max    48865.000000  54050.000000  3877.500000  \n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Read the CSV file with proper settings\n",
    "df = pd.read_csv('database_v3.csv', \n",
    "                 sep=';',  # Use semicolon as separator\n",
    "                 decimal=',',  # Use comma as decimal separator\n",
    "                 parse_dates=['date'])  # Parse the date column\n",
    "\n",
    "# Convert numeric columns to float\n",
    "numeric_columns = df.columns.drop('date')\n",
    "df[numeric_columns] = df[numeric_columns].astype(float)\n",
    "\n",
    "# Now we can calculate z-scores\n",
    "commodity_columns = df.columns.drop('date').tolist()\n",
    "z_scores = np.abs(stats.zscore(df[commodity_columns]))\n",
    "outlier_threshold = 3  # Z-score threshold for outlier detection\n",
    "outliers = df[z_scores > outlier_threshold]\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows of the DataFrame:\")\n",
    "print(df[commodity_columns].head())\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(df[commodity_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d051de",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "This section focuses on detecting anomalies in the commodity time series data. We use the Isolation Forest algorithm, which is an unsupervised learning method particularly effective for anomaly detection in high-dimensional datasets.\n",
    "\n",
    "**Isolation Forest:**\n",
    "Isolation Forest isolates anomalies by randomly partitioning the data. Anomalies are easier to isolate and therefore require fewer partitions. The `contamination` parameter specifies the expected proportion of outliers in the dataset.  It's important to choose this parameter carefully based on your understanding of the data.  A higher value will result in more data points being flagged as anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ad406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from contextlib import contextmanager\n",
    "import signal\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutError(f\"Timed out after {seconds} seconds\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "def detect_anomalies(\n",
    "    df: pd.DataFrame, \n",
    "    commodity: str, \n",
    "    contamination: float = 0.05, \n",
    "    timeout: int = 300\n",
    ") -> tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Detects anomalies in a given commodity's time series data using Isolation Forest.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing commodity time series data\n",
    "        commodity: Name of the commodity column to analyze\n",
    "        contamination: Expected proportion of outliers in the dataset\n",
    "        timeout: Maximum time in seconds to run the model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (anomalies, confidence_scores) where:\n",
    "            - anomalies is a Series with 1 (normal) or -1 (anomaly)\n",
    "            - confidence_scores is a Series with anomaly scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Print debugging information\n",
    "        print(f\"\\nProcessing {commodity}\")\n",
    "        print(f\"Initial shape: {df[commodity].shape}\")\n",
    "        print(f\"Number of NaN values: {df[commodity].isna().sum()}\")\n",
    "        \n",
    "        # Extract the data and reshape\n",
    "        data = df[commodity].values.reshape(-1, 1)\n",
    "        \n",
    "        # Create and fit the imputer\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        data_clean = imputer.fit_transform(data)\n",
    "        \n",
    "        # Validate data\n",
    "        if np.isnan(data_clean).sum() > 0:\n",
    "            raise ValueError(\"Data still contains NaN values after imputation\")\n",
    "        if len(data_clean) == 0:\n",
    "            raise ValueError(\"Empty dataset provided\")\n",
    "        if np.isinf(data_clean).any():\n",
    "            raise ValueError(\"Dataset contains infinite values\")\n",
    "        \n",
    "        # Train Isolation Forest model\n",
    "        model = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            max_samples=min(256, len(data_clean)),\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        # Fit and predict with timeout protection\n",
    "        with time_limit(timeout):\n",
    "            predictions = model.fit_predict(data_clean)\n",
    "        \n",
    "        # Create output series with proper names\n",
    "        anomalies = pd.Series(\n",
    "            predictions, \n",
    "            index=df.index, \n",
    "            name=f\"{commodity}_anomalies\"\n",
    "        )\n",
    "        \n",
    "        # Calculate normalized confidence scores (0 to 1 range)\n",
    "        raw_scores = model.score_samples(data_clean)\n",
    "        normalized_scores = (raw_scores - raw_scores.min()) / (raw_scores.max() - raw_scores.min())\n",
    "        confidence_scores = pd.Series(\n",
    "            normalized_scores,\n",
    "            index=df.index,\n",
    "            name=f\"{commodity}_confidence\"\n",
    "        )\n",
    "        \n",
    "        return anomalies, confidence_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {commodity}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa310cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing gold\n",
      "Initial shape: (5803,)\n",
      "Number of NaN values: 5\n",
      "\n",
      "Processing crude\n",
      "Initial shape: (5803,)\n",
      "Number of NaN values: 5\n",
      "\n",
      "Processing brent\n",
      "Initial shape: (5803,)\n",
      "Number of NaN values: 5\n",
      "\n",
      "crude:\n",
      "Total anomalies detected: 290\n",
      "Average confidence score: 0.838\n",
      "\n",
      "brent:\n",
      "Total anomalies detected: 290\n",
      "Average confidence score: 0.854\n"
     ]
    }
   ],
   "source": [
    "# Process single commodity\n",
    "anomalies, confidence = detect_anomalies(df, \"gold\")\n",
    "\n",
    "# Process multiple commodities\n",
    "results = {}\n",
    "for commodity in commodity_columns:\n",
    "    anomalies, confidence = detect_anomalies(df, commodity)\n",
    "    if anomalies is not None:\n",
    "        results[commodity] = {\n",
    "            'anomalies': anomalies,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "# Print summary\n",
    "for commodity, result in results.items():\n",
    "    anomaly_count = (result['anomalies'] == -1).sum()\n",
    "    print(f\"\\n{commodity}:\")\n",
    "    print(f\"Total anomalies detected: {anomaly_count}\")\n",
    "    print(f\"Average confidence score: {result['confidence'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddb722",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8981268d",
   "metadata": {},
   "source": [
    "## Ensemble Clustering\n",
    "\n",
    "This section implements ensemble clustering on the identified anomaly regions using k-means, DBSCAN, and hierarchical clustering.\n",
    "\n",
    "**Why Ensemble Clustering?**\n",
    "Ensemble clustering combines the results of multiple clustering algorithms to obtain a more robust and stable clustering solution.  Different algorithms have different strengths and weaknesses, and combining them can help to overcome the limitations of individual algorithms. In this case, we are using K-means, DBSCAN and Hierarchical clustering. However, the results are not combined, and each algorithm is evaluated independently.\n",
    "\n",
    "It evaluates the quality of the clusters using Silhouette score and Davies-Bouldin index, and analyzes the characteristics of each cluster.\n",
    "\n",
    "**Clustering Methods:**\n",
    "*   **K-means:** A centroid-based clustering algorithm that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.\n",
    "*   **DBSCAN:** A density-based clustering algorithm that groups together points that are closely packed together, marking as outliers points that lie alone in low-density regions.\n",
    "*   **Hierarchical Clustering:** A clustering algorithm that builds a hierarchy of clusters. It starts with each data point in its own cluster, and then merges the closest pairs of clusters until only a single cluster remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf2e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 19:26:44.578086: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-14 19:26:45.218131: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-14 19:26:45.220175: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-14 19:26:46.390601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Evaluation Results:\n",
      "K-means - Silhouette: 0.694, Davies-Bouldin: 0.731\n",
      "DBSCAN - Silhouette: -1.000, Davies-Bouldin: -1.000\n",
      "Hierarchical - Silhouette: 0.670, Davies-Bouldin: 0.731\n",
      "\n",
      "Cluster Characteristics:\n",
      "\n",
      "K-means:\n",
      "Cluster 0:\n",
      "  Size: 4\n",
      "  Mean length: 111.00\n",
      "  Mean value: 27.38\n",
      "  Std value: 51.15\n",
      "Cluster 1:\n",
      "  Size: 2\n",
      "  Mean length: 111.00\n",
      "  Mean value: 94.48\n",
      "  Std value: 56.04\n",
      "Cluster 2:\n",
      "  Size: 43\n",
      "  Mean length: 111.00\n",
      "  Mean value: 2.53\n",
      "  Std value: 13.78\n",
      "\n",
      "DBSCAN:\n",
      "Cluster -1:\n",
      "  Size: 42\n",
      "  Mean length: 111.00\n",
      "  Mean value: 9.52\n",
      "  Std value: 31.37\n",
      "Cluster 0:\n",
      "  Size: 7\n",
      "  Mean length: 111.00\n",
      "  Mean value: 1.07\n",
      "  Std value: 11.22\n",
      "\n",
      "Hierarchical:\n",
      "Cluster 0:\n",
      "  Size: 3\n",
      "  Mean length: 111.00\n",
      "  Mean value: 78.28\n",
      "  Std value: 61.78\n",
      "Cluster 1:\n",
      "  Size: 43\n",
      "  Mean length: 111.00\n",
      "  Mean value: 2.53\n",
      "  Std value: 13.78\n",
      "Cluster 2:\n",
      "  Size: 3\n",
      "  Mean length: 111.00\n",
      "  Mean value: 21.22\n",
      "  Std value: 46.24\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create anomaly regions dictionary\n",
    "anomaly_regions = {}\n",
    "for commodity in commodity_columns:\n",
    "    if commodity not in results:\n",
    "        continue\n",
    "        \n",
    "    # Get anomaly indices\n",
    "    anomaly_indices = np.where(results[commodity]['anomalies'] == -1)[0]\n",
    "    \n",
    "    # Group consecutive indices into regions\n",
    "    regions = []\n",
    "    start = None\n",
    "    for i in range(len(anomaly_indices)):\n",
    "        if start is None:\n",
    "            start = anomaly_indices[i]\n",
    "        elif anomaly_indices[i] - anomaly_indices[i-1] > 1:\n",
    "            regions.append((start, anomaly_indices[i-1]))\n",
    "            start = anomaly_indices[i]\n",
    "    \n",
    "    if start is not None:\n",
    "        regions.append((start, anomaly_indices[-1]))\n",
    "    \n",
    "    anomaly_regions[commodity] = regions\n",
    "\n",
    "# Prepare data for clustering\n",
    "anomaly_data = []\n",
    "for commodity in commodity_columns:\n",
    "    if commodity not in anomaly_regions:\n",
    "        continue\n",
    "    regions = anomaly_regions[commodity]\n",
    "    for start, end in regions:\n",
    "        # Extract the data for the anomaly region\n",
    "        region_data = df[commodity][start:end+1].values\n",
    "        if len(region_data) > 0:  # Only add non-empty regions\n",
    "            anomaly_data.append(region_data)\n",
    "\n",
    "if len(anomaly_data) > 0:\n",
    "    # Pad the sequences to have the same length\n",
    "    padded_anomaly_data = pad_sequences(anomaly_data, padding='post', dtype='float64')\n",
    "\n",
    "    # Define the clustering algorithms\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "    # Fit the clustering algorithms\n",
    "    kmeans_labels = kmeans.fit_predict(padded_anomaly_data)\n",
    "    dbscan_labels = dbscan.fit_predict(padded_anomaly_data)\n",
    "    hierarchical_labels = hierarchical.fit_predict(padded_anomaly_data)\n",
    "\n",
    "    # Evaluate the clustering algorithms\n",
    "    def evaluate_clustering(labels, data):\n",
    "        # Filter out noise labels (-1 for DBSCAN)\n",
    "        if -1 in labels:\n",
    "            mask = labels != -1\n",
    "            if sum(mask) < 2:  # Need at least 2 non-noise points\n",
    "                return -1, -1\n",
    "            data = data[mask]\n",
    "            labels = labels[mask]\n",
    "        if len(set(labels)) < 2:\n",
    "            return -1, -1\n",
    "        silhouette = silhouette_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data, labels)\n",
    "        return silhouette, davies_bouldin\n",
    "\n",
    "    # Evaluate and print results\n",
    "    kmeans_silhouette, kmeans_davies_bouldin = evaluate_clustering(kmeans_labels, padded_anomaly_data)\n",
    "    dbscan_silhouette, dbscan_davies_bouldin = evaluate_clustering(dbscan_labels, padded_anomaly_data)\n",
    "    hierarchical_silhouette, hierarchical_davies_bouldin = evaluate_clustering(hierarchical_labels, padded_anomaly_data)\n",
    "\n",
    "    print(\"\\nClustering Evaluation Results:\")\n",
    "    print(f\"K-means - Silhouette: {kmeans_silhouette:.3f}, Davies-Bouldin: {kmeans_davies_bouldin:.3f}\")\n",
    "    print(f\"DBSCAN - Silhouette: {dbscan_silhouette:.3f}, Davies-Bouldin: {dbscan_davies_bouldin:.3f}\")\n",
    "    print(f\"Hierarchical - Silhouette: {hierarchical_silhouette:.3f}, Davies-Bouldin: {hierarchical_davies_bouldin:.3f}\")\n",
    "\n",
    "    # Analyze cluster characteristics\n",
    "    print(\"\\nCluster Characteristics:\")\n",
    "    for algorithm, labels in [(\"K-means\", kmeans_labels), \n",
    "                            (\"DBSCAN\", dbscan_labels), \n",
    "                            (\"Hierarchical\", hierarchical_labels)]:\n",
    "        print(f\"\\n{algorithm}:\")\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            cluster_data = padded_anomaly_data[labels == label]\n",
    "            print(f\"Cluster {label}:\")\n",
    "            print(f\"  Size: {len(cluster_data)}\")\n",
    "            print(f\"  Mean length: {np.mean([len(x[~np.isnan(x)]) for x in cluster_data]):.2f}\")\n",
    "            print(f\"  Mean value: {np.nanmean(cluster_data):.2f}\")\n",
    "            print(f\"  Std value: {np.nanstd(cluster_data):.2f}\")\n",
    "else:\n",
    "    print(\"No anomaly regions found to cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281169b",
   "metadata": {},
   "source": [
    "## Interactive Dashboard\n",
    "\n",
    "This section creates an interactive dashboard to visualize the time series data, detected anomalies, and clustering results.\n",
    "\n",
    "The dashboard allows the user to select a commodity from a dropdown menu and view the corresponding time series data, anomaly regions, and clustering results.  The anomaly regions are highlighted in red, and the clusters are color-coded to distinguish them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e2725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set the renderer to browser\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "# Define color schemes\n",
    "colors = {\n",
    "    'normal': '#1f77b4',  # Blue\n",
    "    'anomaly': '#d62728',  # Red\n",
    "    'clusters': ['#2ca02c', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']  # Different colors for clusters\n",
    "}\n",
    "\n",
    "# Create plots for each commodity\n",
    "for commodity in commodity_columns:\n",
    "    # Create the figure with two subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(\n",
    "            f'{commodity} Price with Anomaly Regions',\n",
    "            'Confidence Scores and Clusters'\n",
    "        ),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # Add time series data to first subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['date'],\n",
    "            y=df[commodity],\n",
    "            mode='lines',\n",
    "            name=commodity,\n",
    "            line=dict(color=colors['normal'])\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add confidence scores to second subplot\n",
    "    if commodity in results:\n",
    "        confidence_scores = results[commodity]['confidence']\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['date'],\n",
    "                y=confidence_scores,\n",
    "                mode='lines',\n",
    "                name='Confidence Score',\n",
    "                line=dict(color='lightgray')\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Add anomaly regions and cluster information\n",
    "    if commodity in anomaly_regions:\n",
    "        for i, (start, end) in enumerate(anomaly_regions[commodity]):\n",
    "            # Get cluster label\n",
    "            cluster_label = kmeans_labels[i % len(kmeans_labels)]\n",
    "            cluster_color = colors['clusters'][cluster_label % len(colors['clusters'])]\n",
    "            \n",
    "            # Add anomaly region to first subplot\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df['date'][start:end+1],\n",
    "                    y=df[commodity][start:end+1],\n",
    "                    mode='lines',\n",
    "                    name=f'Anomaly (Cluster {cluster_label})',\n",
    "                    line=dict(color=cluster_color, width=3),\n",
    "                    showlegend=True\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add highlighted region to confidence plot\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df['date'][start:end+1],\n",
    "                    y=confidence_scores[start:end+1],\n",
    "                    mode='lines',\n",
    "                    name=f'Cluster {cluster_label}',\n",
    "                    line=dict(color=cluster_color, width=3),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title=dict(\n",
    "            text=f'{commodity} Analysis Dashboard',\n",
    "            x=0.5,\n",
    "            y=0.95\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Date', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Price', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Confidence Score', row=2, col=1)\n",
    "    \n",
    "    # Show the figure - this will open in your default web browser\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
